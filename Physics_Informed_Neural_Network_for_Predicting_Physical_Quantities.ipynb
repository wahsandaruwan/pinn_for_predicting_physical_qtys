{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "Ucxjz_9F2hwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas is a powerful library used for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# numpy is a library for numerical computations and handling arrays/matrices\n",
        "import numpy as np\n",
        "\n",
        "# torch is the main package for deep learning with PyTorch\n",
        "import torch\n",
        "# nn (neural networks) is a subpackage in PyTorch to define and train models\n",
        "import torch.nn as nn\n",
        "# optim is used to implement optimization algorithms such as Adam\n",
        "import torch.optim as optim\n",
        "\n",
        "# train_test_split from scikit-learn splits data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# StandardScaler is used to standardize (scale) features in the dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# drive is used to mount Google Drive in a Google Colab environment\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "EptJ7oIc2jV9"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive"
      ],
      "metadata": {
        "id": "RqpKc2RK3RDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive to access files stored on it in Google Colab\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxznm_Ly3UYs",
        "outputId": "db8153b3-027c-413b-f8b6-0dc975145371"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read data"
      ],
      "metadata": {
        "id": "69ihS0lZ20CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path to the CSV file located in Google Drive\n",
        "file_path = '/content/drive/My Drive/PINN: Blending Physics with Machine Learning/Project 01/Data/100_Data.csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "1RLYwfka22Oi"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify input data"
      ],
      "metadata": {
        "id": "qYq_Ut6774Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Check for Infinity values\n",
        "print(np.isinf(data).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WD9pPj776Au",
        "outputId": "d5496454-34f5-48d5-c020-adc3d9903792"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H        0\n",
            "alpha    0\n",
            "P        0\n",
            "C        0\n",
            "R        0\n",
            "G        0\n",
            "F        0\n",
            "dtype: int64\n",
            "H        0\n",
            "alpha    0\n",
            "P        0\n",
            "C        0\n",
            "R        0\n",
            "G        0\n",
            "F        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the feature matrix (X) and target variable (y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qisEA3Ur3xW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X: The feature matrix\n",
        "# Selecting specific columns from the dataset 'data' to serve as features\n",
        "# 'alpha', 'P', 'G', 'R', 'C', 'H' are the independent variables used to predict the target\n",
        "X = data[['alpha', 'P', 'G', 'R', 'C', 'H']]\n",
        "\n",
        "# y: The target variable\n",
        "# Selecting the column 'F' from the dataset 'data' as the dependent variable to be predicted\n",
        "y = data['F']\n",
        "\n",
        "# Purpose:\n",
        "# - `X` contains the input features required by the model for training and testing.\n",
        "# - `y` contains the output variable that the model will learn to predict."
      ],
      "metadata": {
        "id": "zIYEOKdV3zqK"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into training and testing datasets"
      ],
      "metadata": {
        "id": "4-i-hILR31NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X: The feature matrix (independent variables)\n",
        "# y: The target variable (dependent variable)\n",
        "\n",
        "# train_test_split: A function from sklearn.model_selection\n",
        "# test_size=0.2: Specifies that 20% of the dataset will be used as the test set\n",
        "# The remaining 80% will be used as the training set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Output:\n",
        "# X_train: Feature data for training\n",
        "# X_test: Feature data for testing\n",
        "# y_train: Target data for training\n",
        "# y_test: Target data for testing\n",
        "\n",
        "# Purpose:\n",
        "# This splits the dataset into separate training and testing subsets to evaluate the model's performance.\n",
        "# The test set is kept separate to simulate unseen data and prevent overfitting."
      ],
      "metadata": {
        "id": "0B9RGpLV38o9"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize training and testing datasets"
      ],
      "metadata": {
        "id": "pfTc40rO4Ci6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# StandardScaler scales data to have a mean of 0 and a standard deviation of 1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training feature data (X_train) and transform it\n",
        "# The scaler calculates the mean and standard deviation from X_train\n",
        "# Then it uses these statistics to standardize X_train\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test feature data (X_test) using the same scaler\n",
        "# The scaler uses the mean and standard deviation calculated from X_train\n",
        "# This ensures that the test data is scaled consistently with the training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit the scaler to the training target data (y_train) and transform it\n",
        "# The target values are reshaped to a 2D array as required by StandardScaler\n",
        "# Then it standardizes y_train using its mean and standard deviation\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "# Transform the test target data (y_test) using the same scaler\n",
        "# Like X_test, y_test is scaled using the statistics from y_train\n",
        "# This maintains consistency between training and test target data\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "p-28WpCZ4GE2"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch tensors using training and testing datasets"
      ],
      "metadata": {
        "id": "IvNa9t3k4PCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the scaled training feature data (X_train_scaled) to a PyTorch FloatTensor\n",
        "# This conversion is necessary because PyTorch models work with tensors as input\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "\n",
        "# Convert the scaled training target data (y_train_scaled) to a PyTorch FloatTensor\n",
        "# The target variable needs to be in tensor format for compatibility with PyTorch's loss functions\n",
        "y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
        "\n",
        "# Convert the scaled test feature data (X_test_scaled) to a PyTorch FloatTensor\n",
        "# These tensors will be used for model evaluation and predictions\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "\n",
        "# Convert the scaled test target data (y_test_scaled) to a PyTorch FloatTensor\n",
        "# This allows the evaluation of the model's predictions against the actual test targets\n",
        "y_test_tensor = torch.FloatTensor(y_test_scaled)"
      ],
      "metadata": {
        "id": "t2v5Ln7c4Xu2"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class for defining the physics informed neural network"
      ],
      "metadata": {
        "id": "wCWJJPwAP7QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is a subclass of nn.Module, allowing it to utilize PyTorch's features for defining and training neural networks\n",
        "class PhysicsInformedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class nn.Module\n",
        "        super(PhysicsInformedNN, self).__init__()\n",
        "\n",
        "        # Define the first fully connected (linear) layer\n",
        "        # Input features: 6 (number of input parameters, e.g., alpha, P, G, R, C, H)\n",
        "        # Output features: 50 (hidden layer neurons for learning complex representations)\n",
        "        self.fc1 = nn.Linear(6, 50)\n",
        "\n",
        "        # Define the second fully connected (linear) layer\n",
        "        # Input features: 50 (from the previous layer)\n",
        "        # Output features: 30 (fewer neurons in this layer to reduce complexity)\n",
        "        self.fc2 = nn.Linear(50, 30)\n",
        "\n",
        "        # Define the third fully connected (linear) layer\n",
        "        # Input features: 30 (from the previous layer)\n",
        "        # Output features: 1 (final output, e.g., the predicted F value)\n",
        "        self.fc3 = nn.Linear(30, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the first fully connected layer followed by a ReLU activation function\n",
        "        # ReLU introduces non-linearity to learn complex patterns in the data\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Pass the result through the second fully connected layer followed by ReLU\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        # Pass the result through the third fully connected layer to get the final output\n",
        "        # No activation function here, as the output is expected to be a continuous value\n",
        "        return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "ndh6l5jbP9o2"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the model, loss function and optimizer"
      ],
      "metadata": {
        "id": "gSbsWxEp4jPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Physics-Informed Neural Network (PINN) model\n",
        "# This model incorporates physics-based equations into its architecture or loss function\n",
        "model = PhysicsInformedNN()\n",
        "\n",
        "# Define the Mean Squared Error (MSE) loss function\n",
        "# This measures the average squared difference between predicted and true values\n",
        "# Used as the primary loss function to train the model\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer for training the model\n",
        "# Adam optimizer is chosen for its ability to handle sparse gradients and adaptive learning rates\n",
        "# The learning rate is set to 0.001, which determines the step size during optimization\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "28sVs95s4psL"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for calculating the physics informed loss"
      ],
      "metadata": {
        "id": "QagdiyVh4_rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def physics_loss(alpha, P, G, R, C, H, F):\n",
        "    # Small value to prevent numerical instabilities like division by zero\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    # Convert angles from degrees to radians for trigonometric calculations\n",
        "    alpha_rad = alpha * np.pi / 180\n",
        "    phi_rad = P * np.pi / 180\n",
        "\n",
        "    # Calculate parameter A using an exponential decay model\n",
        "    # Clamp alpha to ensure it stays within a stable range\n",
        "    A = 10.50 * torch.exp(-0.009 * alpha.clamp(min=epsilon, max=1e2))\n",
        "\n",
        "    # Calculate parameter B using a quadratic relationship with alpha\n",
        "    B = 0.72 - (3.5e-5) * alpha**2 + 0.0031 * alpha\n",
        "\n",
        "    # Calculate the tangent of phi and alpha, clamping to avoid extreme values\n",
        "    tan_phi = torch.tan(phi_rad).clamp(min=epsilon, max=1e6)\n",
        "    tan_alpha = torch.tan(alpha_rad).clamp(min=epsilon, max=1e6)\n",
        "\n",
        "    # Compute the right-hand side (rhs) of the physics-informed equation\n",
        "    rhs = (\n",
        "        # First term: scaled by parameter A and raised to power B\n",
        "        A * ((C / (G * H * tan_phi + epsilon)).clamp(min=epsilon, max=1e6)**B) * tan_phi +\n",
        "        # Second term: ratio of tangents\n",
        "        (tan_phi / tan_alpha) -\n",
        "        # Third term: a combination of scaled coefficients and trigonometric terms\n",
        "        ((C / (G * H * tan_alpha + epsilon)).clamp(min=epsilon, max=1e6) +\n",
        "         (tan_phi / (torch.sin(alpha_rad) * torch.cos(phi_rad) + epsilon)))\n",
        "        * R\n",
        "    )\n",
        "\n",
        "    # The left-hand side (lhs) is the observed value, F\n",
        "    lhs = F\n",
        "\n",
        "    # Compute the loss as the mean squared error (MSE) between lhs and rhs\n",
        "    # Clamp the result to avoid extreme loss values\n",
        "    return torch.mean((lhs - rhs).clamp(min=-1e6, max=1e6)**2)"
      ],
      "metadata": {
        "id": "hzWq4ANV4qQv"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the physics informed neural network"
      ],
      "metadata": {
        "id": "Vg-SnEFZ5Gn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs for training\n",
        "num_epochs = 1000\n",
        "\n",
        "# Loop over the specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Zero the gradients of the model parameters (to prevent accumulation from previous iterations)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass: calculate predicted output by passing the input data through the model\n",
        "    y_pred = model(X_train_tensor)\n",
        "\n",
        "    # Calculate the Mean Squared Error (MSE) loss between predicted and true values\n",
        "    mse = mse_loss(y_pred, y_train_tensor)\n",
        "\n",
        "    # Calculate the Physics-Informed Loss using the custom loss function\n",
        "    phys = physics_loss(X_train_tensor[:, 0], X_train_tensor[:, 1], X_train_tensor[:, 2],\n",
        "                        X_train_tensor[:, 3], X_train_tensor[:, 4], X_train_tensor[:, 5], y_pred)\n",
        "\n",
        "    # Combine the MSE loss and Physics-Informed loss with a weight for the physics loss (0.01 here)\n",
        "    total_loss = mse + 0.01 * phys  # The weight (0.01) can be adjusted based on the importance of physics loss\n",
        "\n",
        "    # Backward pass: compute gradients for all parameters based on the total loss\n",
        "    total_loss.backward()\n",
        "\n",
        "    # Apply gradient clipping to avoid exploding gradients (keeping the gradients within a limit)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Update the model parameters using the optimizer (after calculating gradients)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss values every 100 epochs for monitoring training progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], MSE Loss: {mse.item():.4f}, Physics Loss: {phys.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_YyYjaO5H1v",
        "outputId": "e7078016-d671-47a1-98fa-cbd3eb5456a6"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], MSE Loss: 768938.0625, Physics Loss: 163926802432.0000\n",
            "Epoch [200/1000], MSE Loss: 768907.6875, Physics Loss: 163926802432.0000\n",
            "Epoch [300/1000], MSE Loss: 768877.3125, Physics Loss: 163926802432.0000\n",
            "Epoch [400/1000], MSE Loss: 768846.6250, Physics Loss: 163926802432.0000\n",
            "Epoch [500/1000], MSE Loss: 768815.8750, Physics Loss: 163926818816.0000\n",
            "Epoch [600/1000], MSE Loss: 768785.0000, Physics Loss: 163926818816.0000\n",
            "Epoch [700/1000], MSE Loss: 768753.8750, Physics Loss: 163926818816.0000\n",
            "Epoch [800/1000], MSE Loss: 768722.5625, Physics Loss: 163926818816.0000\n",
            "Epoch [900/1000], MSE Loss: 768690.6875, Physics Loss: 163926818816.0000\n",
            "Epoch [1000/1000], MSE Loss: 768658.3750, Physics Loss: 163926818816.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "rqo87eNmQjo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for inference (saves memory and computation)\n",
        "with torch.no_grad():\n",
        "    # Perform a forward pass on the test data to get predictions\n",
        "    y_pred = model(X_test_tensor)\n",
        "\n",
        "    # Calculate the Mean Squared Error (MSE) loss between predicted and true values on the test set\n",
        "    mse = mse_loss(y_pred, y_test_tensor)\n",
        "\n",
        "    # Print the test MSE to evaluate model performance\n",
        "    print(f'Test MSE: {mse.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmYkucUQlKT",
        "outputId": "5f2ad181-2998-4da3-a5b1-80b41439c508"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 208688.8438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual and predicted values"
      ],
      "metadata": {
        "id": "MlFGCSrgQr_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print header for actual vs predicted values comparison\n",
        "print(\"\\nActual vs Predicted Values:\")\n",
        "print(\"Actual\\t\\tPredicted\")\n",
        "\n",
        "# Loop through the actual values and the predicted values to display them\n",
        "# zip() pairs the actual and predicted values together for iteration\n",
        "for actual, predicted in zip(y_test, y_pred.numpy().flatten()):\n",
        "    # Print each pair of actual and predicted values, formatted to 4 decimal places\n",
        "    print(f\"{actual:.4f}\\t\\t{predicted:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BSzEz_2QuK0",
        "outputId": "966b6962-cf91-4ab1-831b-5cdc798b8d8f"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Actual vs Predicted Values:\n",
            "Actual\t\tPredicted\n",
            "0.4740\t\t461.1270\n",
            "0.9320\t\t454.5906\n",
            "3.5510\t\t456.9528\n",
            "2.0420\t\t431.7238\n",
            "0.5650\t\t434.9383\n",
            "1.6420\t\t440.7958\n",
            "1.5550\t\t437.9480\n",
            "0.8720\t\t485.0512\n",
            "0.5080\t\t453.7557\n",
            "0.6830\t\t463.9534\n",
            "1.8180\t\t445.4341\n",
            "1.0530\t\t438.8531\n",
            "0.3880\t\t515.1255\n",
            "0.9140\t\t458.8711\n",
            "1.6440\t\t430.3163\n",
            "0.3020\t\t487.0869\n",
            "0.3530\t\t484.5428\n",
            "0.9030\t\t449.3995\n",
            "0.5550\t\t449.1223\n",
            "1.0120\t\t437.2605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance (approximation based on gradient)"
      ],
      "metadata": {
        "id": "Tp0vHLQfS-DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store feature importance values\n",
        "feature_importance = []\n",
        "\n",
        "# Loop through each feature in the training set\n",
        "for i in range(X_train_tensor.shape[1]):\n",
        "    # Create a clone of the training set tensor to avoid modifying the original data\n",
        "    X_temp = X_train_tensor.clone()\n",
        "    # Set requires_grad=True so we can compute gradients for each feature\n",
        "    X_temp.requires_grad = True\n",
        "\n",
        "    # Perform a forward pass through the model to get the predictions\n",
        "    y_pred = model(X_temp)\n",
        "\n",
        "    # Compute the gradients for the sum of the predictions with respect to the features\n",
        "    y_pred.sum().backward()\n",
        "\n",
        "    # Append the absolute mean of the gradients for the i-th feature to the list\n",
        "    # This represents the importance of that feature based on the gradient magnitude\n",
        "    feature_importance.append(X_temp.grad[:, i].abs().mean().item())\n",
        "\n",
        "# Create a DataFrame to store feature names and their corresponding importance values\n",
        "feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by feature importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "# Print the feature importance values in a readable format\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc0LwySJS_s1",
        "outputId": "7a5baeb6-4dbb-4473-967b-dcc02055e024"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance:\n",
            "  feature  importance\n",
            "1       P   16.605259\n",
            "2       G   15.972275\n",
            "4       C   14.132663\n",
            "5       H   11.641267\n",
            "0   alpha   10.712636\n",
            "3       R    5.623215\n"
          ]
        }
      ]
    }
  ]
}